\chapter{Implementazione} \label{chap:implementation}

In questo capitolo verrà descritta l'implementazione del software sviluppato per l'elaborazione di immagini lunari. In particolare, verranno presentate le scelte progettuali e le soluzioni adottate per la realizzazione delle funzionalità di calibrazione, allineamento, pre-processing, stacking e post-processing affrontate nel \cref{chap:techniques}. I frammenti di codice riportati in questo capitolo rappresentano solo le parti più significative del software; in molti casi sono state omesse linee di controllo o operazioni di supporto per ragioni di brevità. Per una visione completa si rimanda al repository \href{https://github.com/Spina02/Moon-Stacker.git}{GitHub} del progetto.

\section{Architettura del software} \label{sec:architecture}

Il software è stato sviluppato in linguaggio Python, utilizzando principalmente le librerie OpenCV e NumPy per la manipolazione delle immagini.


\begin{multicols}{2}

    Il progetto segue una struttura modulare, con un file principale \texttt{main.py} che coordina l'esecuzione delle funzioni dei vari moduli. L'inizializzazione delle variabili e dei parametri avviene nel file \texttt{config.py}, che contiene la definizione di alcuni flag (come \texttt{DEBUG}), i percorsi delle cartelle utilizzate (input, output, bias, dark, flat, ecc.) e alcune funzioni per l'inizializzazione delle variabili e delle metriche. La pipeline di elaborazione delle immagini è descritta nel file \texttt{process.py}, che si occupa di chiamare le funzioni di calibrazione (\texttt{calibration.py}), allineamento (\texttt{align.py}), denoising (\texttt{denoise.py}), stacking (\texttt{stacking.py}) e, infine, le operazioni di pre-processing e post-processing (\texttt{enhancement.py}).

    \columnbreak

    \begin{verbatim}
         
        src/
        |-- align.py
        |-- analysis.py
        |-- calibration.py
        |-- config.py
        |-- denoise.py
        |-- enhancement.py
        |-- grid_search.py
        |-- image.py
        |-- main.py
        |-- metrics.py
        |-- model.pth
        |-- process.py
        |-- stacking.py
        `-- utils.py
    \end{verbatim}

\end{multicols}

Il modulo \texttt{utils.py} contiene alcune funzioni di ausiliarie (ad esempio, per creare cartelle di destinazione se non esistenti). All'interno di \texttt{image.py} sono presenti funzioni di utilità per la manipolazione delle immagini, come il caricamento, il salvataggio e la visualizzazione. Inoltre, nel file \texttt{metrics.py} sono presenti funzioni per il calcolo di metriche come il contrasto e la luminosità di un'immagine, e metriche di qualità come \texttt{BRISQUE}, \texttt{NIQE} e \texttt{LIQE}, affrontate più nel dettaglio nel \cref{chap:evaluation}

Nella prossima sezione verrà illustrata la pipeline completa di elaborazione delle immagini.

\section{Pipeline di elaborazione} \label{sec:pipeline_impl}

La pipeline di elaborazione completa è stata implementata nel file \texttt{process.py}. Questo file contiene la funzione \texttt{process\_images}, che esegue l'intera pipeline di elaborazione delle immagini, dalla calibrazione all'allineamento, dallo stacking al post-processing. La funzione prende in input una lista di immagini \texttt{images} e un dizionario di parametri \texttt{params} contenente i parametri di configurazione per la pipeline, e restituisce l'immagine finale elaborata.

\begin{lstlisting}
    def process_images(images=None, params={}):
        det_str    = params.get('det_str',    1.3)
        grad_thr   = params.get('grad_thr',   0.008)
        dns_str    = params.get('dns_str',    1.2)
        stack_alg  = params.get('stack_alg',  'weighted_avg')
        avg_alg    = params.get('avg_alg',    'sharpness')
        ush_str    = params.get('ush_str',    2.35)
        tile_size  = params.get('tile_size',  (19, 19))
        clip_limit = params.get('clip_limit', 0.8)

        # Calibration
        calibrated = calibrate_images(images)

        # Alignment 
        aligned = align_images(images)

        # Pre-processing
        cropped = crop_to_center(aligned)
        denoised = custom_unsharp_mask(aligned, det_str, grad_thr, dns_str)
        no_bg = [soft_threshold(img, 0.05, 50) for img in denoised]

        # Stacking
        stacked = stack_images(no_bg, stack_alg, average_alg)

        # Post-processing
        contrasted = enhance_contrast(stacked, clip_limit, tile_size)
        unsharped = unsharp_mask(contrasted, unsharp_strength)
        final_image = shades_of_gray(unsharped)
        
        return final_image
\end{lstlisting}

I parametri di default sono stati ottenuti eseguendo una grid search (implementata in \texttt{grid\_search.py}) su un paio di set di immagini di test, al fine di trovare i valori ottimali che massimizzano la qualità dell'output finale secondo le metriche implementate in \texttt{metrics.py}. Tale ricerca ha permesso di identificare una configurazione bilanciata di parametri che garantisce risultati accettabili sui diversi set.

Nelle sezioni seguenti verranno illustrate in dettaglio le implementazioni delle singole funzionalità della pipeline, esaminando le scelte progettuali e gli algoritmi utilizzati per ciascun modulo.

\subsection{Calibrazione} \label{subsec:calibration_impl}

La fase di calibrazione è stata implementata nel file \texttt{calibration.py}, dove sono definiti i metodi per calcolare i \textit{master bias}, \textit{dark} e \textit{flat}, e quello per applicare la calibrazione alle immagini.

I metodi per calcolare i master seguono la procedura descritta negli algoritmi \ref{alg:bias}, \ref{alg:dark} e \ref{alg:flat} e sono riportati di seguito:

\begin{lstlisting}
    [label={lst:calculate_masters}]
    # Function to calculate the master bias
    def calculate_master_bias(bias):
        # Calculate the mean
        return np.mean(bias, axis=0)

    # Function to calculate the master dark
    def calculate_master_dark(dark, master_bias=None):
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        # Subtract the master bias, then calculate the mean
        return np.mean(dark - master_bias, axis=0)

    # Function to calculate the master flat
    def calculate_master_flat(flat, master_bias=None, master_dark=None):
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        if master_dark is None: master_dark = np.zeros_like(flat[0])
        # Subtract master bias and dark, then calculate the mean
        master_flat = np.mean(flat-master_bias-master_dark, axis=0)   
        # Normalize the master flat
        mean_flat = np.mean(master_flat)
        if mean_flat != 0: master_flat /= mean_flat
        return master_flat
\end{lstlisting}

Queste funzioni calcolano rispettivamente i master bias, dark e flat, utilizzando la media su tutti i frame di calibrazione disponibili. Se i master bias o dark non sono disponibili, vengono inizializzati a matrici di zeri della stessa dimensione delle immagini di calibrazione. Nel caso di \texttt{calculate\_master\_flat}, dopo aver sottratto il master bias e il master dark, si normalizza il master flat dividendo per la sua media, a condizione che questa sia diversa da zero.

Le funzioni tre funzioni sopra definite vengono chiamate da una funzione ausiliaria (\texttt{calculate\_masters}) che, prima di calcolare i masters, verifica se i frame di calibrazione sono stati caricati correttamente e se hanno le stesse dimensioni delle immagini da calibrare; in caso contrario, viene sollevata un'eccezione. Inoltre, se per un tipo di frames di calibrazione il numero di immagini disponibili è inferiore a una certa soglia \texttt{MIN\_CALIBRATION} specificata nel file di configurazione, il master corrispondente non viene calcolato e viene stampato un messaggio di avviso, poiché con troppi pochi frames il calcolo potrebbe non essere affidabile e introdurre nuovi artefatti.

Questa fase risulta compiutazionalmente onerosa, in quanto richiede il caricamento di tutti i frame di calibrazione in memoria, e il calcolo dei master implica l'elaborazione di ogni singolo pixel di ogni frame. Per questo motivo, è stata implementata la possibilità di salvare i master calcolati su file, in modo da poterli riutilizzare senza doverli ricalcolare ogni volta.

Il metodo per calibrare una singola immagine fa riferimento all'\cref{alg:calibration}. L'implementazione è la seguente:

\begin{lstlisting}
    # Function to calibrate a single image
    def calibrate_single_image(image, master_bias = None, master_dark = None, master_flat = None):
        if master_bias is None and master_dark is None and master_flat is None:
            print("No calibration masters available: skipping")
            return image
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        if master_dark is None: master_dark = np.zeros_like(flat[0])
        if master_flat is None: master_flat = np.ones_like(flat[0])
        # Calibrate the image
        calibrated = (image - master_bias - master_dark)/master_flat
        calibrated = np.clip(calibrated, 0, 1)  # Clip to valid range
        return calibrated
\end{lstlisting}

Nella funzione \texttt{calibrate\_single\_image}, si procede alla calibrazione dell'immagine sottraendo il master bias e il master dark, e dividendo per il master flat. Se uno dei master non è disponibile, viene inizializzato a una matrice di zeri (per bias e dark) o di uni (per flat) della stessa dimensione dell'immagine. In questo modo, sia nel calcolo dei master, sia nella calibrazione di una singola immagine, è possibile non utilizzare uno o più set di frame di calibrazione nel caso in cui non siano disponibili. Infine, si utilizza la funzione \texttt{np.clip} di \texttt{NumPy} per assicurarsi che i valori dell'immagine risultante siano compresi nell'intervallo [0, 1].

\subsection{Allineamento} \label{subsec:alignment_impl}

La fase di allineamento delle immagini è stata implementata nel file \texttt{align.py}. Qui sono definite le funzioni necessarie per allineare le immagini utilizzando algoritmi di \textit{feature detection} e \textit{feature matching} come ORB, SIFT e SURF, e RANSAC per il calcolo della trasformazione omografica, come già affrontatp nella \cref{subsec:feature_detection}. L'allineamento è fondamentale per compensare eventuali spostamenti o rotazioni tra gli scatti, garantendo una sovrapposizione precisa delle immagini durante la fase di \textit{stacking}.

Per migliorare i risultati dell'allineamento, le immagini passano prima attraverso una funzione di pre-processing che ne ottimizza la qualità prima dell'estrazione delle feature. Questa funzione è stata implementata nel file \texttt{enhancement.py} e svolge essenzialmente tre compiti:

\begin{enumerate}
    \item \textbf{Convertire la foto in scala di grigi} (se non lo è già): riducendo l'immagine a un solo canale, si alleggerisce il carico computazionale per l'estrazione dei keypoint e dei descrittori, oltre a rendere il processo più robusto a variazioni cromatiche.
    
    \item \textbf{Rimozione dello sfondo}: per facilitare l'estrazione delle features, si è scelto di rimuovere lo sfondo dell'immagine, che potrebbe contenere rumore o dettagli non rilevanti. Questo passaggio è stato implementato tramite la funzione \texttt{soft\_threshold} (\cref{alg:remove_background}), che applica una sogliatura morbida all'immagine, mantenendo solo i pixel con intensità superiore ad una certa soglia \texttt{thr}.
    
    \item \textbf{Miglioramento del contrasto}: questo passaggio, effettuato tramite l'applicazione dell'\cref{alg:clache}, è utile per migliorare la qualità dell'immagine, rendendo più nitidi i dettagli e facilitando l'estrazione delle features.
\end{enumerate}

Inoltre, poiché gli algoritmi di OpenCV richiedono immagini in formato a 8 bit, è stata implementata una funzione \texttt{to\_8bit} che converte l'immagine in un formato a 8 bit, normalizzando i valori dei pixel nell'intervallo [0, 255].

L'implementazione dell'algoritmo è riportata di seguito, mentre le implementazioni di \texttt{soft\_threshold} e \texttt{enhance\_contrast}  sono riportate nella \cref{subsec:preprocessing_impl}.

\begin{lstlisting}[label={lst:pre_align_enhance}]
    def pre_align_enhance(image, clip_limit = 0.8, tile_grid_size = (20,20), thr = 0.05):
        # Convert image to grayscale
        enhanced = image.copy()
        if len(image.shape) == 3:
            enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
        # Remove background using a threshold
        enhanced = soft_threshold(enhanced, thr)
        # Enhance contrast using CLACHE
        enhanced = enhance_contrast(enhanced, clip_limit, tile_grid_size)
        enhanced = to_8bit(enhanced)
        return enhanced
\end{lstlisting}

La funzione principale per l'allineamento è \texttt{align\_images}. Tale funzione prende in input una lista di immagini \texttt{images} e restituisce una lista di immagini allineate \texttt{aligned}. I due parametri opzionali \texttt{method} e \texttt{nfeatures} permettono di specificare l'algoritmo di feature detection da utilizzare (tra \texttt{orb}, \texttt{sift} e \texttt{surf}) e il numero di features da estrarre. I valori di default sono rispettivamente \texttt{orb} e \texttt{5000}.

La funzione \texttt{align\_images} inizialmente crea un oggetto \texttt{detector} e un oggetto \texttt{matcher} in base all'algoritmo scelto per effettuare l'estrazione e il matching delle feature. Inizializza la lista \texttt{aligned\_images} con la prima immagine della lista, che fungerà da immagine di riferimento. Si procede quindi a migliorare quest'ultima tramite la funzione \texttt{pre\_align\_enhance}, sopra riportata. Si estraggono quindi i keypoint e i descrittori dell'immagine di riferimento, che saranno necessari per calcolare l'allineamento di tutte le altre immagini utilizzando la funzione \texttt{align\_image}. Infine, si restituisce la lista di immagini allineate.

L'implementazione è la seguente:

\begin{lstlisting}[label={lst:alignment}]
    def align_images(images, method='orb', nfeatures=5000):
        # Choose the feature detection algorithm
        if method == 'orb':
            detector = cv2.ORB_create(nfeatures=nfeatures)
        elif method == 'sift':
            detector = cv2.SIFT_create(nfeatures=nfeatures)            
        elif method == 'surf':
            detector = cv2.xfeatures2d.SURF_create()
        # Choose the norm for the matcher
        norm = cv2.NORM_HAMMING if method == 'orb' else cv2.NORM_L2
        # Create a matcher object
        matcher = cv2.BFMatcher.create(norm)

        # The first image is the reference image
        ref_image = images[0]
        aligned_images = [ref_image]

        # Enhance the reference image to improve alignment
        enhanced_ref = pre_align_enhance(ref_image)

        # Detect keypoints and descriptors for the reference image
        ref_kp, ref_des = detector.detectAndCompute(enhanced_ref, None)
        ref_shape = (ref_image.shape[1], ref_image.shape[0])

        # Align each image to the reference image
        for idx, image in enumerate(images[1:]):
            aligned_image = align_image(image, enhanced_ref, ref_kp, ref_des, ref_shape, detector, matcher)
            aligned_images.append(aligned_image)

        return aligned_images
\end{lstlisting}

La funzione \texttt{align\_image} funzione prende in input l'immagine da allineare \texttt{image}, i keypoint \texttt{ref\_kp} e i descrittori \texttt{ref\_des} dell'immagine di riferimento, l'oggetto \texttt{detector} e il \texttt{matcher}, e restituisce l'immagine allineata con quella di riferimento.

Per allineare l'immagine, si procede come segue:

\begin{enumerate}
    \item Si applica la funzione \texttt{pre\_align\_enhance} all'immagine da allineare, per migliorarne la qualità.
    \item Si estraggono i keypoints e i descrittori dell'immagine da allineare utilizzando il \texttt{detector} scelto.
    \item Si effettua il matching dei descrittori tra l'immagine di riferimento e quella da allineare.
    \item Si applica il \textit{ratio test} per filtrare i buoni match.
    $$
    \text{matches} = \{m \in \text{matches} \mid m.\text{distance} < 0.75 \cdot n.\text{distance}\}
    $$
    \item Si calcola l'omografia tra i keypoints dell'immagine di riferimento e quelli dell'immagine da allineare. Nel mio caso ho scelto di utilizzare interpolazione \texttt{INTER\_LANCZOS4}, molto utilizzata in astrofotografia per la sua capacità di preservare i dettagli.
    \item Si applica la trasformazione omografica all'immagine da allineare.
\end{enumerate}

L'implementazione di \texttt{align\_image} è riportata qui sotto:

\begin{lstlisting}[label={lst:single_alignment}]
    def align_image(image, ref_kp, ref_des, detector, matcher):
        # Enhance the image for alignment process
        aligned_image = pre_align_enhance(image)

        # Find keypoints and descriptors for the image
        kp, des = detector.detectAndCompute(aligned_image, None)

        # Match the descriptors
        matches = matcher.knnMatch(ref_des, des, k=2)
        
        # Apply the ratio test
        matches = [m for m, n in matches if m.distance < 0.75 * n.distance]
        
        # Compute the homography
        ref_pts = np.float32([ref_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        img_pts = np.float32([kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        H, _ = cv2.findHomography(img_pts, ref_pts, cv2.RANSAC, 10.0, maxIters=3000, confidence=0.995)
        shape = image.shape[1], image.shape[0]
        
        # Warp the original image using the final homography
        aligned_image = cv2.warpPerspective(image, H, shape, flags=cv2.INTER_LANCZOS4)
        return aligned_image
\end{lstlisting}

I parametri \texttt{maxIters} e \texttt{confidence} nell'invocazione di \texttt{cv2.findHomography} permettono di regolare il numero massimo di iterazioni e la confidenza richiesta per il calcolo della trasformazione omografica. Questi parametri sono stati scelti in modo da ottenere un buon compromesso tra accuratezza e velocità di esecuzione.

\subsection{Pre-processing} \label{subsec:preprocessing_impl}

La fase di pre-processing affrontata nella \cref{sec:preprocessing} è stata implementata nel file \texttt{enhancement.py}. Questa fase comprende operazioni volte a migliorare la qualità delle immagini calibrate e allineate, preparandole per il processo di stacking. Le operazioni implementate vengono eseguite a cascata e sono:

\begin{itemize}
    \item \textbf{Ritaglio dell immagine} per centrare il soggetto ed eliminare bordi esterni indesiderati
    \item \textbf{Miglioramento della nitidezza} attraverso l'uso di tecniche di Unsharp Mask personalizzate e DnCNN;
    \item \textbf{Rimozione dello sfondo} tramite la sogliatura morbida;
\end{itemize}

Di seguito sono descritte le funzioni implementate per effettuare tali operazioni.

\textbf{Ritaglio dell'immagine}

Il ritaglio dell'immagine, basato sull'\cref{alg:crop}, è stato implementato nella funzione \texttt{crop\_to\_center}. Questa funzione prende in input una lista di immagini allineate \texttt{images} e un margine \texttt{margin} (di default 10 pixel) e restituisce una lista di immagini ritagliate in modo che il soggetto sia centrato e i bordi esterni siano eliminati.

\begin{lstlisting}[label={lst:crop}]
    def crop_to_center(images, margin=10):
    cropped_images = []

    # Process the first image to get the cropping parameters
    ref = images[0]
    gray = ref
    if len(ref.shape) == 3:
        gray = cv2.cvtColor(gray, cv2.COLOR_RGB2GRAY)

    # Binary thresholding
    _, thresh = cv2.threshold(gray, 0.1, 1.0, cv2.THRESH_BINARY)

    # Find contours
    contours, _ = cv2.findContours(to_8bit(thresh), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contour = max(contours, key=cv2.contourArea)

    # Get the bounding rectangle of the selected contour
    x, y, w, h = cv2.boundingRect(contour)
    center_x, center_y = x + w//2, y + h//2 # Calculate the center
    size = max(w, h) + 2 * margin               # Determine the size

    # Calculate the top-left corner of the square
    start_x = max(center_x - size//2, 0)
    start_y = max(center_y - size//2, 0)

    # Ensure the square fits within the image boundaries
    end_x = min(start_x + size, ref.shape[1])
    end_y = min(start_y + size, ref.shape[0])

    # Crop all images using the same parameters
    for image in images:
        cropped_image = image[start_y:end_y, start_x:end_x]
        cropped_image = np.clip(cropped_image, 0, 1)
        cropped_images.append(cropped_image)

    return cropped_images
\end{lstlisting}

Da notare che la funzione \texttt{crop\_to\_center} ritaglia tutte le immagini della lista \texttt{images} utilizzando i parametri di ritaglio calcolati sull'immagine di riferimento. Questo approccio richiede che il soggetto sia centrato in tutte le immagini, altrimenti potrebbero verificarsi problemi di allineamento durante il processo di stacking.

\textbf{Riduzione del rumore e Unsharp Masking con DnCNN}

Come descritto nelle sezioni \ref{subsec:denoising} e \ref{subsec:unsharp_mask}, è stata implementata una versione personalizzata dell'Unsharp Masking che combina la riduzione del rumore tramite DnCNN e l'utilizzo di una maschera basata sul gradiente dell'immagine.

Iniziamo col descrivere l'operazione di denoising con il modello preaddestrato. Per importarlo nel progetto è stato necessario scaricare il modello preaddestrato e salvarlo in un file \texttt{.pth}. Il modello è stato poi caricato utilizzando la libreria \texttt{torch} e la funzione \texttt{torch.load}. Il modello è stato quindi utilizzato per ridurre il rumore delle immagini, come mostrato nel seguente codice.

\begin{lstlisting}[label={lst:dncnn}]
    DNCNN_MODEL_PATH = './src/model.pth'
    
    class DnCNN(nn.Module):
        def __init__(self,depth=17,n_channels=64,image_channels=1):
            super().__init__()
            self.dncnn = nn.Sequential(
                nn.Conv2d(image_channels, n_channels, 3, padding=1), nn.ReLU(inplace=True),
                *[layer_block(n_channels) for _ in range(depth-2)],
                nn.Conv2d(n_channels, image_channels, 3, padding=1)
            )
            self._initialize_weights()
    
        def forward(self, x):
            return x - self.dncnn(x)
    
        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    init.orthogonal_(m.weight)
                    if m.bias is not None: init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    init.constant_(m.weight, 1); init.constant_(m.bias, 0)
    
    def layer_block(n_channels):
        return nn.Sequential(
            nn.Conv2d(n_channels,n_channels,3,padding=1,bias=False),
            nn.BatchNorm2d(n_channels, eps=1e-4, momentum=0.95),
            nn.ReLU(inplace=True)
        )
    
    def model_init(model_path=DNCNN_MODEL_PATH):
        model = torch.load(model_path)
        model.eval()
        return model
\end{lstlisting}

La classe \texttt{DnCNN} implementa una rete neurale convoluzionale profonda progettata specificamente per la riduzione del rumore nelle immagini. Questa rete, denominata \textbf{Denoising Convolutional Neural Network (DnCNN)}, è caratterizzata da una struttura composta da 17 strati convoluzionali, progettati per stimare e rimuovere il rumore presente nell'immagine di input, come visto nella \cref{subsec:denoising}. 

La funzione \texttt{model\_init} si occupa di caricare il modello preaddestrato da un file \texttt{.pth} e restituirlo pronto per l'elaborazione.

Per garantire la compatibilità con il modello DnCNN, le immagini devono essere trasformate in tensori e normalizzate. Dopo il denoising, è necessario riconvertire i tensori in immagini nel formato \texttt{np.float32}. Queste operazioni sono state incapsulate in due funzioni:

\begin{enumerate}
    \item \textbf{\texttt{prepare\_tensor\_for\_model}}: converte un'immagine nel formato tensoriale e la normalizza.

    \item \textbf{\texttt{convert\_tensor\_to\_image}}: riconverte il tensore prodotto dal modello in un'immagine \texttt{np.float32} utilizzabile nei flussi di elaborazione successivi.
\end{enumerate}

L'implementazione di queste funzioni non è riportata per questioni di brevità.

La funzione per effettuare il denoising, basata sull'\cref{alg:dncnn} è quindi la seguente:

\begin{lstlisting}[label={lst:denoise}]
    def perform_denoising(model, image):
        shape = image.shape

        if len(shape) < 3:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        channels = list(cv2.split(color.rgb2lab(image)))
        
        # convert image to tensor
        l = prepare_tensor_for_model(channels[0], 100).to(device)
        
        if torch.cuda.is_available(): device = torch.device('cuda')
        else: torch.device('cpu')
        model = model.to(device).eval()
        with torch.no_grad():
            denoised_l = model(l).cpu()

        # convert tensor to image
        denoised_l = convert_tensor_to_image(denoised_l, 100)
        channels[0] = denoised_l

        # Merge the channels
        denoised = cv2.merge(channels)
        denoised = color.lab2rgb(denoised)

        if len(shape) < 3:
            denoised = cv2.cvtColor(denoised, cv2.COLOR_RGB2GRAY)

        return denoised
\end{lstlisting}

Come è possibile notare, il denoising viene eseguito solo sul canale L dell'immagine, in quanto è il canale che contiene la maggior parte delle informazioni di luminosità. Dopodiché l'immagine viene riconvertita in RGB e, se necessario, in scala di grigi.

Possiamo ora descrivere l'algoritmo di Unsharp Masking personalizzato, basato sull'\cref{alg:custom_unsharp}, che è stato implementato nella funzione \texttt{custom\_unsharp\_mask}. Questa implementazione combina il denoising tramite DnCNN con una maschera basata sul gradiente dell'immagine per preservare i dettagli importanti.

\begin{lstlisting}[label={lst:unsharp_mask}]
    def custom_unsharp_mask(images, model, det_str=1.3, thr=0.09, dns_str=1.2):
        sharpened_images = []

        for idx, image in enumerate(images):
            # Apply denoising with DnCNN
            denoised = denoise(image, model)
            
            # Blend original and denoised image
            denoised = denoised * dns_str + image * (1 - dns_str)

            # Convert to grayscale for gradient calculation if needed
            gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if image.ndim == 3 else image
            
            # Calculate image gradient magnitude
            grad_mag = np.sqrt(np.gradient(gray)**2, axis=0)

            # Create denoise mask: 1 where gradient is low
            mask = np.where(grad_mag < thr, 1, 0).astype(np.float32)
            # Smooth mask edges
            mask = cv2.GaussianBlur(mask, (3,3), 0.5)

            # Expand mask to 3 channels for RGB images
            if image.ndim == 3:
                mask = np.repeat(mask[:, :, np.newaxis], 3, axis=2)

            # Create detail mask (inverse of denoise mask)
            det_mask = 1 - mask

            # Combine denoised and original image using masks
            final_image = (denoised * mask) + (image * det_mask)

            # Amplify details in high gradient areas
            amplified_details = (image - denoised)@$\,$@*@$\,$@det_str@$\,$@*@$\,$@det_mask
            sharpened_image = final_image + amplified_details

            # Ensure valid pixel range
            sharpened_image = np.clip(sharpened_image, 0, 1)
            sharpened_images.append(sharpened_image)

        return sharpened_images
\end{lstlisting}

La funzione \texttt{custom\_unsharp\_mask} prende in input una lista di immagini \texttt{images}, il modello DnCNN \texttt{model} e i parametri per controllare il processo: \texttt{det\_str} regola l'intensità dell'amplificazione dei dettagli, \texttt{thr} definisce la soglia per distinguere aree di dettaglio da aree uniformi, e \texttt{dns\_str} indica l'intensità del denoising sul risultato. 

Per ciascuna immagine, viene applicato il denoising tramite il modello DnCNN, viene poi calcolato il gradiente, da cui si ricavano due maschere che separano zone ad alto dettaglio da quelle più uniformi. La fase finale combina l'immagine originale con quella denoised utilizzando le maschere create, amplificando i dettagli nelle aree ad alto gradiente e privilegiando l'effetto del denoising nelle altre.

\subsection{Stacking} \label{subsec:stacking_impl}

La fase di stacking delle immagini è stata implementata nel file \texttt{stacking.py}. Come descritto nel \cref{sec:stacking}, lo stacking è una tecnica fondamentale in astrofotografia per migliorare il rapporto segnale-rumore combinando più immagini dello stesso soggetto. Nel progetto sono stati implementati tre dei metodi di stacking più comuni:

\begin{itemize}
    \item \textbf{Media Pesata (Weighted Mean)}: calcola la media pesata delle immagini, assegnando pesi basati su una metrica di qualità, come la nitidezza.
    \item \textbf{Mediana (Median Stacking)}: calcola il valore mediano pixel per pixel delle immagini.
    \item \textbf{Sigma Clipping}: calcola la media pixel per pixel escludendo i pixel che deviano oltre una certa soglia dalla media.
\end{itemize}

Il metodo che ha prodotto i risultati migliori è la media pesata, dove i pesi sono determinati in base alla qualità di ciascuna immagine. L'algoritmo è stato implementato nella funzione \texttt{weighted\_average\_stack}.

\begin{lstlisting}[label={lst:weighted_average_stack}]
    def weighted_average_stack(images, method='sharpness'):
        # Calculate the weights for each image
        weights = calculate_weights(images, method)
        # Zero out the lowest 10% of the weights
        weights[np.argsort(weights)[:len(weights)//10]] = 0
        weights = weights / np.sum(weights)

        n_ch = 1 if len(images[0].shape) == 2 else images[0].shape[2]
        stacked_channels = []
        for i in range(n_ch):
            # Extract the i-th channel from each image
            channel_stack = np.array([cv2.split(img)[i] for img in images])

            # Calculate the weighted sum of the channels
            stacked_channel = np.zeros_like(channel_stack[0], dtype=np.float64)
            for channel, weight in zip(channel_stack, weights):
                # Weighted sum
                stacked_channel += channel * weight

            stacked_channels.append(stacked_channel)

        # Combine the channels into a single image
        stacked_image = cv2.merge(stacked_channels)
        return stacked_image.astype(np.float32)
\end{lstlisting}

La funzione \texttt{weighted\_average\_stack} calcola i pesi per ciascuna immagine in base alla metrica specificata (chiamando la funzione \texttt{calculate\_weights}), normalizza i pesi e calcola la media pesata dei pixel per ciascun canale. Infine, combina i canali in un'unica immagine.

La metrica che ha portato a risultati migliori è stata la nitidezza, ma è possibile utilizzare anche altre metriche.

La funzione \texttt{calculate\_weights} per calcolare i pesi è riportata di seguito:

\begin{lstlisting}[label={lst:calculate_weights}]
    def calculate_weights(images, method='sharpness'):
        if method == 'contrast':
            weights = [calculate_contrast(img) for img in images]
        elif method == 'sharpness':
            weights = [calculate_sharpness(img) for img in images]
        elif method == 'brisque':
            weights = [1 - normalize(calculate_metric(img, 'brisque_matlab'), 0, 100) for img in images]
        elif method == 'liqe':
            weights = [calculate_metric(img, 'liqe') for img in images]
        else:
            raise ValueError("Unknown method: {}".format(method))
        return weights 
\end{lstlisting}

Le metriche disponibili includono la nitidezza \texttt{sharpness}, il contrasto \texttt{contrast}, e le metriche \texttt{BRISQUE} e \texttt{LIQE} di PyIqa. Tutte queste metriche vengono calcolate utilizzando le funzioni definite nel file \texttt{metrics.py}, non riportate per brevità.

Il metodo \texttt{median\_stack} implementa lo stacking per mediana, calcolando il valore mediano pixel per pixel delle immagini. Questo metodo è utile per ridurre l'effetto di outlier e rumore impulsivo.

\begin{lstlisting}[label={lst:median_stack}]
    def median_stack(images):
        n_ch = 1 if len(images[0].shape) == 2 else images[0].shape[2]
        stacked_channels = [np.median([cv2.split(img)[i] for img in images], axis=0) for i in range(n_ch)]
        stacked_image = cv2.merge(stacked_channels)
        return stacked_image.astype(np.float32)
\end{lstlisting}

Il metodo \texttt{sigma\_clipping} implementa lo stacking utilizzando il sigma clipping, che esclude i pixel che deviano oltre una certa soglia (espressa in termini di deviazione standard) dalla media.

\begin{lstlisting}[label={lst:sigma_clipping}]
    def sigma_clipping(images, sigma=3):
        n_ch = 1 if len(images[0].shape) == 2 else images[0].shape[2]
        stacked_channels = []
        for i in range(n_ch):
            channel_stack = np.array([cv2.split(img)[i] for img in images])
            mean = np.mean(channel_stack, axis=0)
            std = np.std(channel_stack, axis=0)
            # Mask for values within @\color{green!40!black} \texttt{sigma $\times$ std}@ from the mean
            mask = np.abs(channel_stack - mean) < sigma * std
            # Clip the values outside the mask to the mean
            clipped = np.where(mask, channel_stack, mean)
            # Calculate the mean of the clipped values
            stacked_channel = np.mean(clipped, axis=0)
            stacked_channels.append(stacked_channel)
        stacked_image = cv2.merge(stacked_channels)
        return stacked_image.astype(np.float32)
\end{lstlisting}

Questo metodo è efficace nel ridurre l'impatto di valori anomali e artefatti.

\subsection{Post-processing} \label{subsec:postprocessing_impl}

La fase di post-processing è stata implementata nel file \texttt{enhancement.py} e comprende diverse operazioni finalizzate a migliorare ulteriormente la qualità dell'immagine dopo lo stacking. Le principali operazioni eseguite sono:

\begin{itemize}
    \item \textbf{Miglioramento della nitidezza} utilizzando l'algoritmo tradizionale di Unsharp Mask.
    \item \textbf{Miglioramento del contrasto} applicando l'algoritmo CLAHE (Contrast Limited Adaptive Histogram Equalization).
    \item \textbf{Bilanciamento del colore} per correggere eventuali dominanti cromatiche presenti nell'immagine finale.
\end{itemize}

Di seguito sono descritte le implementazioni di queste operazioni.

\textbf{Miglioramento della nitidezza con Unsharp Mask}

Dopo lo stacking, può essere utile aumentare ulteriormente la nitidezza dell'immagine per enfatizzare i dettagli superficiali della Luna. L'algoritmo di Unsharp Mask tradizionale è stato utilizzato per questo scopo, implementato nella funzione \texttt{unsharp\_mask}.

\begin{lstlisting}[label={lst:unsharp_mask}]
    def unsharp_mask(image, strength):
        # Apply a Gaussian filter to blur the image
        blurred_image = cv2.GaussianBlur(image, (3, 3), 0.5)

        # Combine the blurred image with the original image
        sharpened_image = cv2.addWeighted(image, 1 + strength, blurred_image, -strength, 0)

        # Clip the values to the valid range
        sharpened_image = np.clip(sharpened_image, 0, 1)

        return sharpened_image
\end{lstlisting}

In questa funzione, viene applicato un filtro Gaussiano per ottenere una versione sfocata dell'immagine. La differenza tra l'immagine originale e quella sfocata viene poi amplificata di un fattore \texttt{strength} e sommata all'originale. Questo approccio, descritto in dettaglio nella \cref{subsec:unsharp_mask}, incrementa efficacemente la nitidezza dell'immagine preservando al contempo la naturalezza dei dettagli e minimizzando l'introduzione di artefatti.

Il parametro \texttt{strength} consente un controllo preciso sull'intensità dell'effetto di sharpening. Attraverso test empirici condotti nell'ambito di questo progetto, è stato determinato che valori intorno a 2.35 forniscono il miglior compromesso tra miglioramento della nitidezza e preservazione della qualità dell'immagine.

\textbf{Miglioramento del contrasto con CLAHE}

Per migliorare il contrasto locale e rendere più visibili le variazioni tonali nell'immagine, è stato utilizzato l'algoritmo CLAHE (Contrast Limited Adaptive Histogram Equalization), descritto nell'\cref{alg:clache}.

L'implementazione è stata realizzata nella funzione \texttt{enhance\_contrast}.

\begin{lstlisting}[label={lst:clache}]
    def enhance_contrast(image, clip_limit = 0.07, tile_size = (20, 20)):
        if len(image.shape) < 3: image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)

        # Convert the image to LAB color space
        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
        l_channel = lab[:, :, 0]

        # Apply CLAHE to the L channel
        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)
        l_channel_equalized = clahe.apply(l_channel)

        # Update the L channel
        lab[:, :, 0] = l_channel_equalized

        # Convert the image back to RGB color space
        enhanced_image = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)

        return enhanced_image
\end{lstlisting}

L'algoritmo migliora il contrasto locale dell'immagine, rendendo più evidenti i dettagli nelle aree scure o chiare. I parametri \texttt{clip\_limit} e \texttt{tile\_grid\_size} permettono di regolare il livello di contrasto e la dimensione dei blocchi utilizzati per il calcolo dell'istogramma. I valori di default sono stati scelti in modo da ottenere risultati ottimali.

\textbf{Bilanciamento del colore}

Per correggere eventuali dominanti cromatiche e bilanciare i colori dell'immagine finale, è stato applicato nuovamente l'algoritmo \texttt{Shades of Gray}, già descritto nella \cref{subsec:color_balance}. L'implementazione è la seguente:

\begin{lstlisting}[label={lst:color_balance}]
    def shades_of_gray(image, power=6):
        # Calculate norm values for each channel using Minkowski norm
        norm_values = np.power(np.mean(np.power(image, power), axis=(0, 1)), 1 / power)
        
        # Calculate the mean of the norm values
        mean_norm = np.mean(norm_values)
        
        # Calculate scaling factors to balance the color channels
        scale_factors = mean_norm / norm_values
        
        # Apply the scaling factors to balance the image
        balanced_image = image * scale_factors
        
        # Clip values to valid range [0,1]
        balanced_image = np.clip(balanced_image, 0, 1)

        return balanced_image 
\end{lstlisting}

Questa funzione normalizza i canali di colore dell'immagine, correggendo le deviazioni cromatiche e garantendo una resa cromatica più naturale.

Le operazioni di post-processing vengono eseguite in sequenza per ottenere un'immagine finale ottimizzata. Dopo lo stacking, l'immagine passa attraverso l'Unsharp Mask per migliorare la nitidezza, CLAHE per aumentare il contrasto e infine Shades of Gray per bilanciare i colori. Questa sequenza garantisce che i dettagli siano ben definiti, il contrasto elevato e i colori equilibrati, migliorando significativamente la qualità visiva dell'immagine finale.

Per assicurare che le operazioni di post-processing funzionino correttamente indipendentemente dalle condizioni iniziali dell'immagine, è stata implementata una gestione delle immagini in scala di grigi e a colori. Ad esempio, se l'immagine è in scala di grigi, viene convertita temporaneamente in formato RGB prima di applicare CLAHE, per evitare errori di dimensionamento dei canali.

\section{Sfide affrontate e soluzioni adottate} \label{sec:challenges}

Durante la fase di progettazione e implementazione del progetto, sono state affrontate diverse sfide. Alcune di esse sono state risolte, mentre altre, a causa di limitazioni di tempo o risorse, rimangono aperte per futuri sviluppi.

\subsection{Alto utilizzo di RAM}

Un problema significativo riscontrato durante lo sviluppo è stato l'elevato utilizzo di memoria RAM da parte del programma. Tale problema è stato causato principalmente dall'uso di numerose immagini ad medio-alta risoluzione, che richiedono molta memoria per essere elaborate. La causa principale risiede nell'implementazione della funzione per la lettura e il caricamento delle immagini in memoria. Questo metodo prende in input il percorso della cartella contenente le foto e le apre tutte contemporaneamente (utilizzando metodi diversi in base al formato), inserendole in una lista.

\begin{lstlisting}[label={lst:read_images}]
    def read_image(file_path):
        if file_path.lower().endswith(('.tiff', '.tif')):
            return imageio.imread(file_path).astype(np.float32)
        if file_path.lower().endswith(raw_formats):
            with rawpy.imread(file_path) as raw:
                # convert to rgb image using the camera white balance
                return to_float32(raw.postprocess(use_camera_wb=True, no_auto_bright=True, output_bps=16))
        else:
            return to_float32(imageio.imread(file_path))

    def read_images(folder_path, max_img=MAX_IMG):
        img_paths = read_folder(folder_path, max_img)
        @\color{red}\textit{\# $\ ------ \downarrow$ here is where the problem lies $\downarrow ------$}@
        images = [read_image(path) for path in img_paths]
\end{lstlisting}

In questo modo, ogni immagine viene caricata in RAM in formato a 32 bit. Lavorando con insiemi di 20-30 di immagini l'uno (ad esempio nella fase di calibrazione sono stati utilizzati 30 bias frames e altrettanti dark frames, per un totale di circa 90 immagini caricate in memoria contemporaneamente) spesso il programma termina in modo anomalo a causa dell'uso eccessivo di memoria. Per ovviare a questo problema, è stata suddivisa la pipeline in stadi intermedi nei quali vengono salvati i risultati parziali in formato TIFF a 32 bit. In questo modo, è possibile riavviare l'esecuzione del programma senza dover ricalcolare tutti gli step precedenti al crash; tuttavia questa non rappresenta una soluzione ottimale.

Una soluzione più adatta a questo problema è quella dell'uso di generatori per leggere le immagini. Ciò permette di caricare in memoria solo un'immagine alla volta, riducendo notevolmente l'uso di RAM. Una possibile implementazione è la seguente:

\begin{lstlisting}[label={lst:image_generator}]
    def read_images_generator(folder_path, max_imgs=None):
        loaded = 0
        for filename in sorted(os.listdir(folder_path)):
            if max_imgs is not None and loaded >= max_imgs:
                break
            img_path = os.path.join(folder_path, filename)
            if os.path.isfile(img_path):
                if img_path.lower().endswith(('.tiff', '.tif')):
                    yield imageio.imread(img_path).astype(np.float32)
                if img_path.lower().endswith(raw_formats):
                    with rawpy.imread(img_path) as raw:
                        # convert to rgb image using the camera wb
                        yield to_float32(raw.postprocess(use_camera_wb=True, no_auto_bright=True, output_bps=16))
                else:
                    yield to_float32(imageio.imread(img_path))
                loaded += 1
\end{lstlisting}

Questo metodo è stato testato per generare le immagini di analisi e ha portato a un notevole risparmio di memoria. Si propone quindi come soluzione per futuri sviluppi.

L'utilizzo intensivo di memoria ha spesso influenzato le scelte di progetto, portando a preferire soluzioni più efficienti dal punto di vista della memoria, anche a discapito della velocità di esecuzione. Un esempio si ha nella scelta dell'immagine di riferimento per l'allineamento; inizialmente si era pensato di selezionare tra le immagini acquisite quella con la migliore qualità, ma ciò avrebbe richiesto il caricamento in memoria di tutte le immagini per confrontarle, con un conseguente aumento dell'uso di RAM. Si è quindi preferito utilizzare la prima immagine come riferimento, anche se ciò potrebbe portare a risultati di allineamento meno precisi nel caso in cui la qualità dell'immagine di riferimento fosse inferiore rispetto alle altre.

\subsection{Ricerca dei parametri ottimali}

Una delle maggiori sfide di questo progetto è stata la ricerca dei parametri ottimali per l'elaborazione delle immagini. Molti parametri dipendono fortemente dalle immagini in ingresso, come la soglia \texttt{grad\_thr} per generare la maschera dal gradiente, o \texttt{dns\_str}, che regola l'intensità del denoising da applicare alla foto, entrambi nella funzione \texttt{custom\_unsharp\_mask}. È quindi necessario effettuare una ricerca avanzata dei parametri per rendere i risultati ottimali indipendentemente da fattori come la luminosità, il contrasto e la quantità di rumore, presenti nelle immagini di partenza. 

Un'alternativa per ottenere risultati più precisi consiste nell'estrarre i valori di tali parametri direttamente dalle caratteristiche delle immagini di partenza. Ad esempio, il parametro \texttt{tile\_size} dipende principalmente dalle dimensioni dell'immagine, più che da caratteristiche della foto in sé; dopo diverse prove, è stato notato che si ottengono risultati migliori per valori che si aggirano attorno a 1/70 delle dimensioni dell'immagine dopo la fase di ritaglio.

Un ulteriore problema riguarda la valutazione dei risultati nella ricerca dei parametri. Quando si ha a che fare anche solo con una decina di parametri da testare, provando tre valori per ciascuno di essi, si ottengono migliaia di foto in output, rendendo l'analisi visiva impossibile. Inoltre, spesso si osservano variazioni tra le immagini non sempre distinguibili ad occhio nudo. È quindi necessario stabilire una metrica di riferimento che rifletta il miglioramento visivo dell'immagine. Da ciò sorge un ulteriore problema: l'assenza di un'immagine di riferimento.

\subsection{Assenza di immagine di riferimento}

Come verrà illustrato nel \cref{chap:evaluation}, la metrica di valutazione maggiormente adottata nel contesto dell'astrofotografia è il rapporto segnale-rumore (SNR). Questa metrica, per essere calcolata, richiede la presenza di un'immagine di riferimento ideale a cui il programma deve tendere. Poiché le immagini sono state acquisite durante il progetto, non era disponibile un'immagine di riferimento ideale. È stato quindi tentato di utilizzare come riferimento un'immagine (scaricabile dai siti INAF o NASA) acquisita dal Lunar Reconnaissance Orbiter (LRO), un satellite in orbita intorno alla Luna. Questo satellite ha compiuto diversi scatti, da cui è stata elaborata l’immagine finale tramite la tecnica dello stitching. Tuttavia, ciò causa problemi di compatibilità con le immagini acquisite. Infatti, la foto di riferimento presenta, nelle aree periferiche della Luna, dettagli che non compaiono nelle immagini acquisite, rendendo impossibile l'allineamento. Sono state inoltre esplorate altre immagini di riferimento, ma quelle ad uso libero trovate su internet spesso non presentano una qualità superiore a quelle utilizzate nel progetto, rendendole quindi inadatte come riferimento “ideale”, oppure sono immagini delle cosiddette "mineral moon", risultanti dalla sovrapposizione di scatti ottenuti attraverso filtri di frequenza, con una colorazione artificiale che le rende non adatte al confronto.

Una tecnica adottata comunemente per risolvere questo problema consiste nel prendere un'immagine di alta qualità e, da essa, generare diversi set di immagini con rumore aggiunto, in modo da simulare le condizioni reali. Questo metodo, chiamato \textit{data augmentation}, è stato adottato in diversi progetti di machine learning, e potrebbe essere una soluzione per questo progetto. Tuttavia, vista la mancanza di tempo e di risorse, non è stato possibile implementare questa soluzione, che si rimanda a sviluppi futuri.

Viste le difficoltà affrontate, è stato deciso di utilizzare delle metriche di valutazione senza riferimento, le quali però, come vedremo nel dettaglio nel prossimo capitolo, non sempre sono in grado di percepire il miglioramento effettivo della foto.

\subsection{Pochi dati per testing}

Come già specificato nel capitolo introduttivo, gli scatti sono stati acquisiti tramite una fotocamera bridge Fujifilm Finepix S1, un dispositivo di media gamma, senza l'ausilio di strumentazione professionale come telescopi o montature robotizzate (descritte nella \cref{subsec:hardware}).

Il problema principale è stato rappresentato dalle condizioni meteorologiche, che nei mesi precedenti la realizzazione del progetto sono state raramente favorevoli, permettendo di acquisire pochi scatti e, soprattutto, con poca variazione dell'illuminazione tra i vari set. Questa limitazione ha complicato la ricerca dei parametri, aumentando il rischio di overfitting sui dati disponibili per i test.

Un altro problema riscontrato è la difficoltà nell'acquisizione dei bias frames. Questi infatti, come già specificato nella \cref{subsec:flat}, richiedono di catturare (nelle stesse condizioni di temperatura dei light frames) una fonte luminosa omogenea, il problema sorge quando si ha a che fare con una fonte non omogenea, come una lampada a incandescenza o a LED, che possono causare vignettatura o illuminazione non uniforme, che, invece di migliorare i risultati, li peggiorerebbero.

\cleardoublepage