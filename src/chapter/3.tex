\chapter{Implementazione} \label{chap:implementation}

In questo capitolo verrà descritta l'implementazione del software sviluppato per l'elaborazione di immagini lunari. In particolare, verranno presentate le scelte progettuali e le soluzioni adottate per la realizzazione delle funzionalità di calibrazione, allineamento, pre-processing, stacking e post-processing affrontate nel \cref{chap:techniques}. I frammenti di codice riportati in questo capitolo rappresentano solo le parti più significative del software; in molti casi sono state omesse linee di controllo o operazioni di supporto per ragioni di brevità. Per una visione completa si rimanda al repository \href{https://github.com/Spina02/Moon-Stacker.git}{GitHub} del progetto.

\section{Architettura del software} \label{sec:architecture}

Il software è stato sviluppato in linguaggio Python, utilizzando principalmente le librerie OpenCV e NumPy per la manipolazione delle immagini.


\begin{multicols}{2}

    Il progetto segue una struttura modulare, con un file principale \texttt{main.py} che coordina l'esecuzione delle funzioni dei vari moduli. L'inizializzazione delle variabili e dei parametri avviene nel file \texttt{config.py}, che contiene la definizione di alcuni flag (come \texttt{DEBUG}), i percorsi delle cartelle utilizzate (input, output, bias, dark, flat, ecc.) e alcune funzioni per l'inizializzazione delle variabili e delle metriche. La pipeline di elaborazione delle immagini è descritta nel file \texttt{process.py}, che si occupa di chiamare le funzioni di calibrazione (\texttt{calibration.py}), allineamento (\texttt{align.py}), denoising (\texttt{denoise.py}), stacking (\texttt{stacking.py}) e, infine, le operazioni di pre-processing e post-processing (\texttt{enhancement.py}).

    \columnbreak

    \begin{verbatim}
         
        src/
        |-- align.py
        |-- analysis.py
        |-- calibration.py
        |-- config.py
        |-- denoise.py
        |-- enhancement.py
        |-- grid_search.py
        |-- image.py
        |-- main.py
        |-- metrics.py
        |-- model.pth
        |-- process.py
        |-- stacking.py
        `-- utils.py
    \end{verbatim}

    \end{multicols}

Il modulo \texttt{utils.py} contiene alcune funzioni di ausiliarie (ad esempio, per creare cartelle di destinazione se non esistenti). All'interno di \texttt{image.py} sono presenti funzioni di utilità per la manipolazione delle immagini, come il caricamento, il salvataggio e la visualizzazione. Inoltre, nel file \texttt{metrics.py} sono presenti funzioni per il calcolo di metriche come il contrasto e la luminosità di un'immagine, e metriche di qualità come \texttt{BRISQUE}, \texttt{NIQE} e \texttt{LIQE}, affrontate più nel dettaglio nel \cref{chap:evaluation}

Di seguito vengono descritte più nel dettaglio le implementazioni delle funzionalità principali del software.

\subsection{Calibrazione} \label{subsec:calibration_impl}

La fase di calibrazione è stata implementata nel file \texttt{calibration.py}, dove sono definiti i metodi per calcolare i \textit{master bias}, \textit{dark} e \textit{flat}, e quello per applicare la calibrazione alle immagini.

I metodi per calcolare i master seguono la procedura descritta negli algoritmi \ref{alg:bias}, \ref{alg:dark} e \ref{alg:flat} e sono riportati di seguito:

\begin{lstlisting}
    [label={lst:calculate_masters}]
    # Function to calculate the master bias
    def calculate_master_bias(bias):
        # Calculate the mean
        return np.mean(bias, axis=0)

    # Function to calculate the master dark
    def calculate_master_dark(dark, master_bias=None):
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        # Subtract the master bias, then calculate the mean
        return np.mean(dark - master_bias, axis=0)

    # Function to calculate the master flat
    def calculate_master_flat(flat, master_bias=None, master_dark=None):
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        if master_dark is None: master_dark = np.zeros_like(flat[0])
        # Subtract master bias and dark, then calculate the mean
        master_flat = np.mean(flat-master_bias-master_dark, axis=0)   
        # Normalize the master flat
        mean_flat = np.mean(master_flat)
        if mean_flat != 0: master_flat /= mean_flat
        return master_flat
\end{lstlisting}

Queste funzioni calcolano rispettivamente i master bias, dark e flat, utilizzando la media su tutti i frame di calibrazione disponibili. Se i master bias o dark non sono disponibili, vengono inizializzati a matrici di zeri della stessa dimensione delle immagini di calibrazione. Nel caso di \texttt{calculate\_master\_flat}, dopo aver sottratto il master bias e il master dark, si normalizza il master flat dividendo per la sua media, a condizione che questa sia diversa da zero.

Le funzioni tre funzioni sopra definite vengono chiamate da una funzione ausiliaria (\texttt{calculate\_masters}) che, prima di calcolare i masters, verifica se i frame di calibrazione sono stati caricati correttamente e se hanno le stesse dimensioni delle immagini da calibrare; in caso contrario, viene sollevata un'eccezione. Inoltre, se per un tipo di frames di calibrazione il numero di immagini disponibili è inferiore a una certa soglia \texttt{MIN\_CALIBRATION} specificata nel file di configurazione, il master corrispondente non viene calcolato e viene stampato un messaggio di avviso, poiché con troppi pochi frames il calcolo potrebbe non essere affidabile e introdurre nuovi artefatti.

Questa fase risulta compiutazionalmente onerosa, in quanto richiede il caricamento di tutti i frame di calibrazione in memoria, e il calcolo dei master implica l'elaborazione di ogni singolo pixel di ogni frame. Per questo motivo, è stata implementata la possibilità di salvare i master calcolati su file, in modo da poterli riutilizzare senza doverli ricalcolare ogni volta.

Il metodo per calibrare una singola immagine fa riferimento all'\cref{alg:calibration}. L'implementazione è la seguente:

\begin{lstlisting}
    # Function to calibrate a single image
    def calibrate_single_image(image, master_bias = None, master_dark = None, master_flat = None):
        if master_bias is None and master_dark is None and master_flat is None:
            print("No calibration masters available: skipping")
            return image
        if master_bias is None: master_bias = np.zeros_like(flat[0])
        if master_dark is None: master_dark = np.zeros_like(flat[0])
        if master_flat is None: master_flat = np.ones_like(flat[0])
        # Calibrate the image
        calibrated = (image - master_bias - master_dark)/master_flat
        calibrated = np.clip(calibrated, 0, 1)  # Clip to valid range
        return calibrated
\end{lstlisting}

Nella funzione \texttt{calibrate\_single\_image}, si procede alla calibrazione dell'immagine sottraendo il master bias e il master dark, e dividendo per il master flat. Se uno dei master non è disponibile, viene inizializzato a una matrice di zeri (per bias e dark) o di uni (per flat) della stessa dimensione dell'immagine. In questo modo, sia nel calcolo dei master, sia nella calibrazione di una singola immagine, è possibile non utilizzare uno o più set di frame di calibrazione nel caso in cui non siano disponibili. Infine, si utilizza la funzione \texttt{np.clip} di \texttt{NumPy} per assicurarsi che i valori dell'immagine risultante siano compresi nell'intervallo [0, 1].

\subsection{Allineamento} \label{subsec:alignment_impl}

La fase di allineamento delle immagini è stata implementata nel file \texttt{align.py}. Qui sono definite le funzioni necessarie per allineare le immagini utilizzando algoritmi di \textit{feature detection} e \textit{feature matching} come ORB, SIFT e SURF, e RANSAC per il calcolo della trasformazione omografica, come già affrontatp nella \cref{subsec:feature_detection}. L'allineamento è fondamentale per compensare eventuali spostamenti o rotazioni tra gli scatti, garantendo una sovrapposizione precisa delle immagini durante la fase di \textit{stacking}.

Per migliorare i risultati dell'allineamento, le immagini passano prima attraverso una funzione di pre-processing che ne ottimizza la qualità prima dell'estrazione delle feature. Questa funzione è stata implementata nel file \texttt{enhancement.py} e svolge essenzialmente tre compiti:

\begin{enumerate}
    \item \textbf{Convertire la foto in scala di grigi} (se non lo è già): riducendo l'immagine a un solo canale, si alleggerisce il carico computazionale per l'estrazione dei keypoint e dei descrittori, oltre a rendere il processo più robusto a variazioni cromatiche.
    
    \item \textbf{Rimozione dello sfondo}: per facilitare l'estrazione delle features, si è scelto di rimuovere lo sfondo dell'immagine, che potrebbe contenere rumore o dettagli non rilevanti. Questo passaggio è stato implementato tramite la funzione \texttt{soft\_threshold} (\cref{alg:remove_background}), che applica una sogliatura morbida all'immagine, mantenendo solo i pixel con intensità superiore ad una certa soglia \texttt{thr}.
    
    \item \textbf{Miglioramento del contrasto}: questo passaggio, effettuato tramite l'applicazione dell'\cref{alg:clache}, è utile per migliorare la qualità dell'immagine, rendendo più nitidi i dettagli e facilitando l'estrazione delle features.
\end{enumerate}

Inoltre, poiché gli algoritmi di OpenCV richiedono immagini in formato a 8 bit, è stata implementata una funzione \texttt{to\_8bit} che converte l'immagine in un formato a 8 bit, normalizzando i valori dei pixel nell'intervallo [0, 255].

L'implementazione dell'algoritmo è riportata di seguito, mentre le implementazioni di \texttt{soft\_threshold} e \texttt{enhance\_contrast}  sono riportate nella \cref{subsec:preprocessing_impl}.

\begin{lstlisting}[label={lst:pre_align_enhance}]
    def pre_align_enhance(image, clip_limit = 0.8, tile_grid_size = (20,20), thr = 0.05):
        # Convert image to grayscale
        enhanced = image.copy()
        if len(image.shape) == 3:
            enhanced = cv2.cvtColor(enhanced, cv2.COLOR_RGB2GRAY)
        # Remove background using a threshold
        enhanced = soft_threshold(enhanced, thr)
        # Enhance contrast using CLACHE
        enhanced = enhance_contrast(enhanced, clip_limit, tile_grid_size)
        enhanced = to_8bit(enhanced)
        return enhanced
\end{lstlisting}

La funzione principale per l'allineamento è \texttt{align\_images}. Tale funzione prende in input una lista di immagini \texttt{images} e restituisce una lista di immagini allineate \texttt{aligned}. I due parametri opzionali \texttt{method} e \texttt{nfeatures} permettono di specificare l'algoritmo di feature detection da utilizzare (tra \texttt{orb}, \texttt{sift} e \texttt{surf}) e il numero di features da estrarre. I valori di default sono rispettivamente \texttt{orb} e \texttt{5000}.

La funzione \texttt{align\_images} inizialmente crea un oggetto \texttt{detector} e un oggetto \texttt{matcher} in base all'algoritmo scelto per effettuare l'estrazione e il matching delle feature. Inizializza la lista \texttt{aligned\_images} con la prima immagine della lista, che fungerà da immagine di riferimento. Si procede quindi a migliorare quest'ultima tramite la funzione \texttt{pre\_align\_enhance}, sopra riportata. Si estraggono quindi i keypoint e i descrittori dell'immagine di riferimento, che saranno necessari per calcolare l'allineamento di tutte le altre immagini utilizzando la funzione \texttt{align\_image}. Infine, si restituisce la lista di immagini allineate.

L'implementazione è la seguente:

\begin{lstlisting}[label={lst:alignment}]
    def align_images(images, method='orb', nfeatures=5000):
        # Choose the feature detection algorithm
        if method == 'orb':
            detector = cv2.ORB_create(nfeatures=nfeatures)
        elif method == 'sift':
            detector = cv2.SIFT_create(nfeatures=nfeatures)            
        elif method == 'surf':
            detector = cv2.xfeatures2d.SURF_create()
        # Choose the norm for the matcher
        norm = cv2.NORM_HAMMING if method == 'orb' else cv2.NORM_L2
        # Create a matcher object
        matcher = cv2.BFMatcher.create(norm)

        # The first image is the reference image
        ref_image = images[0]
        aligned_images = [ref_image]

        # Enhance the reference image to improve alignment
        enhanced_ref = pre_align_enhance(ref_image)

        # Detect keypoints and descriptors for the reference image
        ref_kp, ref_des = detector.detectAndCompute(enhanced_ref, None)
        ref_shape = (ref_image.shape[1], ref_image.shape[0])

        # Align each image to the reference image
        for idx, image in enumerate(images[1:]):
            aligned_image = align_image(image, enhanced_ref, ref_kp, ref_des, ref_shape, detector, matcher)
            aligned_images.append(aligned_image)

        return aligned_images
\end{lstlisting}

La funzione \texttt{align\_image} funzione prende in input l'immagine da allineare \texttt{image}, i keypoint \texttt{ref\_kp} e i descrittori \texttt{ref\_des} dell'immagine di riferimento, l'oggetto \texttt{detector} e il \texttt{matcher}, e restituisce l'immagine allineata con quella di riferimento.

Per allineare l'immagine, si procede come segue:

\begin{enumerate}
    \item Si applica la funzione \texttt{pre\_align\_enhance} all'immagine da allineare, per migliorarne la qualità.
    \item Si estraggono i keypoints e i descrittori dell'immagine da allineare utilizzando il \texttt{detector} scelto.
    \item Si effettua il matching dei descrittori tra l'immagine di riferimento e quella da allineare.
    \item Si applica il \textit{ratio test} per filtrare i buoni match.
    $$
    \text{matches} = \{m \in \text{matches} \mid m.\text{distance} < 0.75 \cdot n.\text{distance}\}
    $$
    \item Si calcola l'omografia tra i keypoints dell'immagine di riferimento e quelli dell'immagine da allineare. Nel mio caso ho scelto di utilizzare interpolazione \texttt{INTER\_LANCZOS4}, molto utilizzata in astrofotografia per la sua capacità di preservare i dettagli.
    \item Si applica la trasformazione omografica all'immagine da allineare.
\end{enumerate}

L'implementazione di \texttt{align\_image} è riportata qui sotto:

\begin{lstlisting}[label={lst:single_alignment}]
    def align_image(image, ref_kp, ref_des, detector, matcher):
        # Enhance the image for alignment process
        aligned_image = pre_align_enhance(image)

        # Find keypoints and descriptors for the image
        kp, des = detector.detectAndCompute(aligned_image, None)

        # Match the descriptors and filter the matches
        matches = matcher.knnMatch(ref_des, des, k=2)
        matches = [m for m, n in matches if m.distance < 0.75 * n.distance]
        
        # Compute the homography
        ref_pts = np.float32([ref_kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        img_pts = np.float32([kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        H, _ = cv2.findHomography(img_pts, ref_pts, cv2.RANSAC, 10.0, maxIters=3000, confidence=0.995)
        shape = image.shape[1], image.shape[0]
        
        # Warp the original image using the final homography
        aligned_image = cv2.warpPerspective(image, H, shape, flags=cv2.INTER_LANCZOS4)
        return aligned_image
\end{lstlisting}

I parametri \texttt{maxIters} e \texttt{confidence} nell'invocazione di \texttt{cv2.findHomography} permettono di regolare il numero massimo di iterazioni e la confidenza richiesta per il calcolo della trasformazione omografica. Questi parametri sono stati scelti in modo da ottenere un buon compromesso tra accuratezza e velocità di esecuzione.

\subsection{Pre-processing} \label{subsec:preprocessing_impl}

La fase di pre-processing è stata implementata nel file \texttt{enhancement.py}. Questa fase comprende operazioni volte a migliorare la qualità delle immagini calibrate e allineate, preparandole per il processo di stacking. Le operazioni implementate son:

\begin{itemize}
    \item \textbf{Ritaglio dell immagine} per centrare il soggetto ed eliminare bordi esterni indesiderati
    \item \textbf{Riduzione del rumore (denoising)} tramite la rete neurale DnCNN;
    \item \textbf{Miglioramento della nitidezza} attraverso l'uso di tecniche di Unsharp Mask personalizzate;
    \item \textbf{Rimozione dello sfondo} tramite la sogliatura morbida;
\end{itemize}

Di seguito sono descritte le funzioni implementate per effettuare tali operazioni.

\textbf{Riduzione del rumore e Unsharp Masking con DnCNN}

Come descritto nel \cref{subsec:denoising} e nel \cref{subsec:unsharp_mask}, è stata implementata una versione personalizzata dell'Unsharp Masking che combina la riduzione del rumore tramite DnCNN e l'utilizzo di una maschera basata sul gradiente dell'immagine.

Iniziamo col descrivere l'operazione di denoising con il modello preaddestrato. Per importarlo nel progetto è stato necessario scaricare il modello preaddestrato e salvarlo in un file \texttt{.pth}. Il modello è stato poi caricato utilizzando la libreria \texttt{torch} e la funzione \texttt{torch.load}. Il modello è stato quindi utilizzato per ridurre il rumore delle immagini, come mostrato nel seguente codice.

\begin{lstlisting}[label={lst:dncnn}]
    DNCNN_MODEL_PATH = './src/model.pth'
    
    class DnCNN(nn.Module):
        def __init__(self,depth=17,n_channels=64,image_channels=1):
            super().__init__()
            self.dncnn = nn.Sequential(
                nn.Conv2d(image_channels, n_channels, 3, padding=1), nn.ReLU(inplace=True),
                *[layer_block(n_channels) for _ in range(depth-2)],
                nn.Conv2d(n_channels, image_channels, 3, padding=1)
            )
            self._initialize_weights()
    
        def forward(self, x):
            return x - self.dncnn(x)
    
        def _initialize_weights(self):
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    init.orthogonal_(m.weight)
                    if m.bias is not None: init.constant_(m.bias, 0)
                elif isinstance(m, nn.BatchNorm2d):
                    init.constant_(m.weight, 1); init.constant_(m.bias, 0)
    
    def layer_block(n_channels):
        return nn.Sequential(
            nn.Conv2d(n_channels,n_channels,3,padding=1,bias=False),
            nn.BatchNorm2d(n_channels, eps=1e-4, momentum=0.95),
            nn.ReLU(inplace=True)
        )
    
    def model_init(model_path=DNCNN_MODEL_PATH):
        model = torch.load(model_path)
        model.eval()
        return model
\end{lstlisting}
    

La classe \texttt{DnCNN} implementa una rete neurale convoluzionale profonda progettata specificamente per la riduzione del rumore nelle immagini. Questa rete, denominata \textbf{Denoising Convolutional Neural Network (DnCNN)}, è caratterizzata da una struttura composta da 17 strati convoluzionali, progettati per stimare e rimuovere il rumore presente nell'immagine di input, come visto nella \cref{subsec:denoising}. 

La funzione \texttt{model\_init} si occupa di caricare il modello preaddestrato da un file \texttt{.pth} e restituirlo pronto per l'elaborazione.

Per garantire la compatibilità con il modello DnCNN, le immagini devono essere trasformate in tensori e normalizzate. Dopo il denoising, è necessario riconvertire i tensori in immagini nel formato \texttt{np.float32}. Queste operazioni sono state incapsulate in due funzioni:

\begin{enumerate}
    \item \textbf{\texttt{prepare\_tensor\_for\_model}}: converte un'immagine nel formato tensoriale e la normalizza.

    \item \textbf{\texttt{convert\_tensor\_to\_image}}: riconverte il tensore prodotto dal modello in un'immagine \texttt{np.float32} utilizzabile nei flussi di elaborazione successivi.
\end{enumerate}

L'implementazione di queste funzioni non è riportata per questioni di brevità.

La funzione per effettuare il denoising, basata sull'\cref{alg:dncnn} è quindi la seguente:

\begin{lstlisting}[label={lst:denoise}]
    def perform_denoising(model, image):
        shape = image.shape

        if len(shape) < 3:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        channels = list(cv2.split(color.rgb2lab(image)))
        
        # convert image to tensor
        l = prepare_tensor_for_model(channels[0], 100).to(device)
        
        if torch.cuda.is_available(): device = torch.device('cuda')
        else: torch.device('cpu')
        model = model.to(device).eval()
        with torch.no_grad():
            denoised_l = model(l).cpu()

        # convert tensor to image
        denoised_l = convert_tensor_to_image(denoised_l, 100)
        channels[0] = denoised_l

        # Merge the channels
        denoised = cv2.merge(channels)
        denoised = color.lab2rgb(denoised)

        if len(shape) < 3:
            denoised = cv2.cvtColor(denoised, cv2.COLOR_RGB2GRAY)

        return denoised
\end{lstlisting}

\subsection{Stacking} \label{subsec:stacking_impl}

\subsection{Post-processing} \label{subsec:postprocessing_impl}

\begin{itemize}
    \item \textbf{Miglioramento della nitidezza} utilizzando l'algoritmo Unsharp Mask tradizionale;
    \item \textbf{Miglioramento del contrasto} applicando l'algoritmo CLAHE;
    \item \textbf{Bilanciamento del colore} per correggere eventuali dominanti cromatiche;
\end{itemize}

\section{Sfide affrontate e soluzioni adottate} \label{sec:challenges}

- Alto utilizzo di RAM 

- No reference image

- Sfondo non completamente nero 



\cleardoublepage